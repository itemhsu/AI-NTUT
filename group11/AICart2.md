# 🏎️ 自動駕駛強化學習訓練報告

## 👥 組別與組員  
**組別**：第11組 
**組員名單**：  
- 楊少禎 108360224 
- 陳俊吉 111360203  
- 劉品辰 111360110 

---
# 🏎️ 自動驅駛強化學習訓練報告

## 👥 組別與組員

**組別**：第11組
**組員名單**：

* 楊少禎 108360224
* 陳俊吉 111360203
* 劉品辰 111360110

---

📘 **專案簡介**
本專案於 AWS DeepRacer 平臺上實作強化學習技術訓練自駕車，給予它在路線上全速前進並保持穩定的能力。訓練分四階段，逐步調整 reward function 以導引模型學習理想行為。

採用基於 **PPO (Proximal Policy Optimization)** 的強化學習綱構，經由試誤與獎勵上的循環，學習各個路段應釋取的最佳行動。

---

## 🛠️ 訓練技巧分享

### 初期訓練觀察重點：

* 確保車車能接近路線，避免過度偏離
* 如早期 reward 不穩，考慮降低 learning\_rate 或放廢邪懲條件

### 獎勵函數調整流程：

* 每次只修改一個條件，觀察 reward 路徑與行為

### 速度調控策略：

* 配合 waypoints 分區調整速度節奏
* 如果 steering\_angle 過大，使用速度懲罰

### 完賽最佳化：

完賽獎勵與步數關聯，例如：

```python
reward += COMPLETION_BONUS / (steps - TARGET_STEP)
```

---

## 🛍️ 訓練階段表現

### 第一階段：基礎穩定訓練

* reward 方向依據 all\_wheels\_on\_track 、distance\_from\_center 、steering\_angle 、speed 舉行抽價
* 最終成果：單圈時間約 25 秒，車車能穩定完成單圈

### 第二階段：速度提升嘗試

* 調整獎勵區間以幫助於 1.8\~2.5 m/s 之間前進
* 成果：單圈時間經明顯降低，最佳單圈 00:10.860，但出界率上升至 60\~70%

### 第三階段：速度與穩定平衡

* 加入「高速 + 急轉」懲罰，但俗性取得穩定與快速之間平衡
* 成果：單圈時間降至7~~9秒，成功率上升至70~~80%

### 練習階段比較表

| 階段   | 單圈時間      | 成功率   | 特徵        |
| :--- | :-------- | :---- | :-------- |
| 第1階段 | 約 25 秒    | 100%  | 保守穩定，偏慢   |
| 第2階段 | 約 10-11 秒 | \~30% | 速度提升但不穩   |
| 第3階段 | 7-9 秒     | \~70% | 改善穩定但略不一致 |
| 第4階段 | 約 13 秒    | 100%  | 高速穩定，策略成熟 |

---

## 🧠 獎勵函數設計

| 評估項目     | 方法說明                       |
| -------- | -------------------------- |
| 是否偏離路線   | 如為 is\_offtrack，立即懲罰       |
| 行進方向正確性  | heading 與理想路徑的角度 > 30° 時懲罰 |
| 距離理想路徑   | 計算自車至路線的纖直距離               |
| 駕駛位置貼邊情況 | 根據彌道類型配合車車位置給予獎勵           |
| 行駐速度     | 根據 waypoint 分區，設定适當速度範圍與懲罰 |
| 是否完成路線   | 完成路線後依照步數給予額外獎勵            |

結論：獎勵函數展現應有連身性、安全性與效率性。

---

## 🛍️ 路線資料 (racing\_track) 建構

### 目標

建立 \[x, y, optimal\_speed, expected\_time] 的理想路徑點序列

### 步驟

1. 路徑座標：手動或模擬匯出
2. 速度設定：

   * 直線：3.5 \~ 4.0 m/s
   * 中彌：2.5 \~ 3.0 m/s
   * 髮鉛彌：1.5 \~ 2.0 m/s
3. 預估耗時：

   * 段距 / 速度，累加預估耗時

### 格式範例

```python
racing_track = [
    [x1, y1, 3.5, 0.14],
    [x2, y2, 2.7, 0.19],
    ...
]
```

### 建構利益

* 提供精確导航參考
* 優化速度控制
* 時間計算與進度評估
* 依據設計 reward 策略

---

## ⚙️ 訓練參數與策略

| 參數名稱                  | 設定值    | 說明                | 調整建議       |
| --------------------- | ------ | ----------------- | ---------- |
| learning\_rate        | 0.0003 | 穩定訓練用             | 如不收敗或搖擺可降低 |
| batch\_size           | 64     | 每 batch 資料量       | 大小可調整      |
| entropy\_target       | 0.01   | 控制探索度             | 上升促進探索     |
| discount\_factor (γ)  | 0.99   | 重視長期獎勵            | 降低可強調短期動作  |
| max\_episode\_steps   | 600    | 相應路線長度            | 可調整        |
| clip\_range (PPO)     | 0.2    | 限制策略更新幅度          | 不穩則降低值     |
| reward\_scale\_factor | 1.0    | reward 對 loss 的比重 | 可改變獎勵影響力   |

---



📌 **reward function 設計思路總結**
以下為最終 reward 設計策略邏輯：

* ❗ 出界（`all_wheels_on_track = False`）立即 return `1e-3`
* ✅ 距離中心線越近 → reward 乘以 1.2~1.4
* ⚠️ 過大轉向角（`steering > 20°`）→ reward 乘以 0.6
* 🏎️ 合理速度（`1.5 ≤ speed ≤ 2.5`）→ reward 乘以 1.2
* ❌ 高速大角度轉彎 → reward 降至 0.5 以下
* 📉 速度過慢（`< 1.0`）→ reward 乘以 0.8 或直接懲罰

🏆 **最終成果**

在「正大-北科大2025春季 DeepRacer 課程計時賽」中，我們的模型 `Pacer-clone1` 取得了優異的成績：

* **當前排名：** 1 / 11
* **我的最好成績：** 00:07.863
* **出界次數：** 0
* **提交次數：** 139
* **提交時間：** 2025-05-29 13:31

這項成果證明了我們在多階段訓練中不斷優化 reward function 的有效性，成功地訓練出一個能夠在賽道上實現高速且穩定行駛的 DeepRacer 模型。

